---
title: 'Lab 8: NLP: POS Tagging and NER'
author: "DO Agoons"
date: "`r Sys.Date()`"
output:
  html_document: default
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# *Lab Overview*

## **Technical Learning Objectives**

-   Understand text parsing and Parts of Speech (POS) tagging
-   Understand the concept of Maximum Entropy (maxent) and how to conduct a POS tagging using various Maxent Annotator options
-   Understand how to interpret the Penn Treebank POS tag codes
-   Become familiar with some of the main NLP packages available for R (such as NLP, openNLP, coreNLP, cleanNLP) and the very powerful integration with Python NLP libraries in spaCy using reticulate and spacyr
-   Understanding NLP in quanteda and tidytext

## **Business Learning Objectives**

-   Understand NLP alternative to the "Bag of Words" approach to text mining
-   Understand what you gain and lose in POS tagging and NLP approaches.
-   Understand the power of Named Entity Recognition (NER) for identifying dates, locations, money, organizations, people, percentages, and much more in a text corpus
-   Understand how to integrate the Python spaCy libraries into R using reticulate and the spacyr package.
-   Understand how to select the NLP packages to use for your needs


### Transitioning to NLP

Lab 8, transitions from statistical text mining approach, using Bag-of-Words (BoW) into a focus on Natural Language Processing (NLP) techniques. We will learn how to parse text and do Parts of Speech (POS) tagging required for NLP. We will use that tagged text to do Named Entity Recognition (NER). There are three main parts to the lab:

- *Part 1: Parts of Speech Tagging*, how to parse text and tagg (split or classify) the relevant parts of speech using openNLP, cleanNLP, and coreNLP.

- *Part 2: Applying Named Entity Recognition*, introduces us to the concept of Maximum Entropy (Maxent) and applies the maxent annotations for persons, locations, organizations, and several others to the corpus. Maxtent is a classification algorithm which has been trained extensively on pos tagging and it classifies text into the various parts of speech. 

- *Part 3: Conducting NLP Analysis with spacyr*, provides an introduction to NLP analysis in the quanteda and tidytext ecosystems. In quanteda and tidytext, NLP requires a tight integration with spacyr, the R environment for the powerful python spaCy libraries.

# **Pre-Lab Instructions**

## Installing Packages, Importing and Exploring Data

Install the following packages: openNLP, sentimentr, coreNLP, cleanNLP, magrittr, NLP, gridExtra, doBy, cshapes, sotu, and spacyr.

```{r}
# install.packages("openNLP")
# install.packages("sentimentr")
# install.packages("coreNLP")
# install.packages("cleanNLP")
# install.packages("magrittr")
# install.packages("NLP")
# install.packages("ggmap")
# install.packages("gridExtra")
# install.packages("ggthemes")
# install.packages("purrr")
# install.packages("doBy")
# install.packages("cshapes")
# install.packages("rJava")
# install.packages("sotu")
# install.packages("spacyr")
# install.packages("tinytex")
# install.packages(pbapply)
```

Install the openNLPmodels.en from an outside repository. Use the install.packages() function, with "openNLPmodels.en" in the argument, but for repos = add ""http://datacube.wu.ac.at/", type = "source". remember to put a comma between the elements of your argument. 

```{r}
# install.packages("openNLPmodels.en", repos = "http://datacube.wu.ac.at/", type = "source")

```

Now, set up your R system environment as follows:

options(stringsAsFactors = FALSE)
Sys.setlocale("LC_ALL","C")
lib.loc = "~/R/win-library/3.2"

```{r}
options(stringsAsFactors = FALSE)
Sys.setlocale("LC_ALL","C")
lib.loc = "~/R/win-library/3.2"

```

Load all of the required libraries.

Begin by loading the following packages: gridExtra, ggmap, and ggthemes. 

These packages must be loaded before loading openNLP because ggplot2 (which will load when ggthemes loads) has a conflict with openNLP using a function called annotate(); and we want to use the function from openNLP (not ggplot2)

```{r}
library(gridExtra)
library(ggmap)
library(ggthemes)
```

Then load the following remaining packages: NLP, openNLP, openNLPmodels.en, pbapply, stringr, rvest, doBy, tm, cshapes, purr, dplyr, spacyr, tinytex.

```{r}
library(NLP)
library(openNLP)
library(openNLPmodels.en) 
library(pbapply)
library(stringr)
library(rvest)
library(doBy)
library(tm)
library(cshapes)
library(purrr)
library(dplyr)
library(spacyr)
library(tinytex) # This package is required to enable R Markdown to Knit to PDF.
```

These are some additional packages you may want to load: rJava, coreNLP, sentimentr, cleanNLP, dplyr, magrittr.

```{r}
library(rJava)
library(coreNLP)
library(sentimentr)
library(cleanNLP)
library(dplyr)
library(magrittr)
```

### Just a note on rJava

When you load rJava, you may have an issue with not having a Java Virtual Machine (JVM) for Java Runtime Environment (JRE) loaded on your computer. If so, you may go to Java.com to download the appropriate version. Here is the link to install on a Mac: <https://www.java.com/en/download/mac_download.jsp>. However, please note, openNLP may not compatible with the most recent version of Java 8 (12), so you may need to use the last stable version of Java 8 (11). This issue may be resolved by the time you work on this lab.

### Preparing for the spacyr part of the lab.

To use spacyr, first install the spacyr package. This process also installs the various dependencies for spacyr, including: RcppTOML, here, and reticulate. The first time you install this package, it may take a little while.

```{r}
# install.packages("spacyr")
```

Now, load the spacyr package.

```{r}
library(spacyr)
```

*The next step is to "initialize" spacyr*. However, before you can initialize spacy, you must complete an install of a miniconda environment. If you would like more detail, the tutorial below will walk you through loading the miniconda environment: <https://spacyr.quanteda.io>.

This gets a little complicated, but essentially, what you need to do is run the following function:

# *This code DOESN'T WORK*
```{r}
# spacy_install()
```

Be forewarned, this function is going to install quite a bit of additional and dependent packages, and will take some time. You definitely want to comment out this function once you have installed spacy and its dependencies.

You should hopefully eventually get a message similar to the following in your console to indicate a successful "initialization of the mini conda environment". 

====================================================================
Download and installation successful
You can now load the package via spacy.load('en_core_web_sm') ~ *english_core_web_small*

Installation complete.
Condaenv: spacy_condaenv: Language model(s): en_core_web_sm
====================================================================

*Once you get this message, you are ready to initialize spacy using the spacy_initialize() function with the following code in the argument: model="en_core_web_sm"*. Note the "model" in the argument is calling the smaller version of the English tokenizer (the larger one is "en_core_web_lg") which you may also use, but it takes a longer time to process.

*Once you initialize spacy, you should see something similiar to the following result in the console:*

======================================================================================================================

Found 'spacy_condaenv'. spacyr will use this environment successfully initialized (spaCy Version: 3.1.3, language model: en_core_web_sm)
(python options: type = "condaenv", value = "spacy_condaenv")

======================================================================================================================

# *This code DOESN'T WORK*
```{r}
# spacy_initialize(model="en_core_web_sm")
```

If your initialization is successful, you are now ready to use the spacyr package for Natural Language Processing (NLP) tasks

*** End of Pre-Lab ***

# **Lab Instructions**

### This lab has 35 exercises. 

```{r}
getwd()
```

# *Part 1: Parts of Speech (POS) Tagging in openNLP*

This section focuses on text parsing, named entity recognition, and understanding a POS tokenizer in the openNLP package

First, let's create some example text to start. Create some text and assign it to object s.
NB: "\n" tells us we're moving to a new line. 
```{r}
s <- paste(c("Pierre Vinken, 61 years old, will join the board as a",
             "nonexecutive director Nov. 29.\n", 
             "Mr. Vinken is chairman of Elsevier, N.V.,  ",
             "the Dutch publishing group."),
           collapse = "")
s
```

Now, convert the above object to a string
```{r}
s <- as.String(s)
s
```

Now, let's create sentence and word token annotations objects called sent_token_annusing the Maxent_Sent_Token_Annotator() function and the Maxent_Word_Token_Annotator() function from the openNLP package. 

```{r}
# This code breaks up the text into individual sentences. it employs the default model for language 'en'.
sent_token_annotator <- Maxent_Sent_Token_Annotator()


# This code segments characters into tokens
word_token_annotator <- Maxent_Word_Token_Annotator()

# When you segment each individual sentence in the text, apply token separation immediately. If you separate it like this, you won't get results
## a2 word_token_annotator)) 

a2 <- annotate(s, list(sent_token_annotator, word_token_annotator))
a2

# After applying the above two annonators in that order, you can then apply other annotators. 
```

```{r}
# Computes POS tag annotations using the Apache OpenNLP Maxent   Part of Speech tagger employing the default model for language 'en'
pos_tag_annotator <- Maxent_POS_Tag_Annotator()


# Annotate our string/text ny the Maxent_POS_Tag_Annotator() [i.e. separate text into parts of speech. This segmentation is applied to an already segmented text ~ text. has been broken up into sentences and into tokens]
a3 <- annotate(s, pos_tag_annotator, a2)
a3


a3w <- subset(a3, type == "word")
a3w

tags <- sapply(a3w$features,"[[","POS")
tags
table(tags)
```

For more information on Parts of Speech (POS) tagging, please see: <https://repository.upenn.edu/cgi/viewcontent.cgi?article=1603&context=cis_reports>; and for a summary, please see: <https://cs.nyu.edu/~grishman/jet/guide/PennPOS.html>.


Now we will extract tokens/POS pairs (all of them):

```{r}
sprintf("%s/%s", s[a3w], tags)
```

# *Part 2: Applying Named Entity Recognition (NER) to the Clinton Email Dataset*

 We will apply the above skills to a subset of the infamous Secretary Clinton emails.

We need to scan a folder in our working directory for multiple files representing the Hillary Clinton emails saved as .txt files. We will be using the list.files function and a wildcard added to the .txt file extension (\*.txt) which will identify any file in that directory that ends in .txt. We will then create an object called "temp" that contains those emails.

Get Data & Organize

```{r}
getwd()
tmp <- list.files(path ="/Users/apple/Documents/MS Analytics ~ 2023/Semester 1 ~ Spring 2023/ITEC 724 ~ Big Data Analytics & Text Mining/LABS/Lab 08/Lab8/C8_final_txts", pattern = '*.txt', full.names = T)

emails <- pblapply(tmp, readLines)
names(emails) <- gsub('.txt', '', list.files(pattern = '.txt'))
```

Let's now examine one (1) email:

```{r}
emails[[1]]
```

Now, let's create a custom function to clean the emails.

```{r}
txtClean <- function(x) {
  x <- x[-1] 
  x <- paste(x,collapse = " ")
  x <- str_replace_all(x, "[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+", "")  # replace do.agoons@yahoo.com by empty space {","}
  x <- str_replace_all(x, "Doc No.", "") # replace the number in the sentence ...Doc No. "C05758905" with blanc space {","}
  x <- str_replace_all(x, "UNCLASSIFIED U.S. Department of State Case No.", "")
  x <- removeNumbers(x)
  x <- as.String(x)
  return(x)
}
```

Now, as a test, we will apply the cleaning function to one email and observe what happens to it.

```{r}
emails[[1]]
txtClean(emails[[1]])[[1]]
```

Now, let's apply the cleaning function *pblapply() * to all emails; and then review one email in the list.

```{r}
allEmails <- pblapply(emails,txtClean)

# What is the meaning of the subscripts??
allEmails[[67]][[1]]

allEmails[[2]][[1]][1]
# Lets examine the 3rd cleaned email from line 18 to 24. In that line, the word "Subject" is found. 
allEmails[[3]][18,24]

# We can also examine the 500th cleaned email from line 1 - 300
allEmails[[500]][1,300]
```
Note that because we have a list, we can't examine a particular email like a document since all emails are have been merged into a some sort of single document. 


To understand NER (named entity recognition), Create POS tagging for *persons*, *locations*, and *organizations*; as well as individual models for sentences, words and parts of speech. This code load the pre-existing feature weights to be called by your R session, but they do not yet apply them to any text.

```{r}
persons            <- Maxent_Entity_Annotator(kind='person')
locations          <- Maxent_Entity_Annotator(kind='location')
organizations      <- Maxent_Entity_Annotator(kind='organization')
sentTokenAnnotator <- Maxent_Sent_Token_Annotator(language='en')
wordTokenAnnotator <- Maxent_Word_Token_Annotator(language='en')
posTagAnnotator    <- Maxent_POS_Tag_Annotator(language='en')
```

## Understanding loops in R

In R, "for loops" iterate for a controlled counter or an index, incremented at each iteration cycle. While "repeat loops" (with conditional clauses) break and next: based on the onset and verification of some logical condition.

We will now annotate each document in a loop

```{r}
annotationsData <- list()
for (i in 1:length(allEmails)){
  print(paste('starting annotations on doc', i))
  annotations <- annotate(allEmails[[i]], list(sentTokenAnnotator, 
                                               wordTokenAnnotator, 
                                               posTagAnnotator, 
                                               persons, 
                                               locations, 
                                               organizations))
  annDF           <- as.data.frame(annotations)[,2:5]
  annDF$features  <- unlist(as.character(annDF$features))
  
  annotationsData[[tmp[i]]] <- annDF
  print(paste('finished annotations on doc', i))
}
```

Annotations have character indices

Now obtain terms by index from each document using a NESTED loop

```{r}
allData<- list()
for (i in 1:length(allEmails)){
  x <- allEmails[[i]]       # get an individual document
  y <- annotationsData[[i]] # get an individual doc's annotation information
  print(paste('starting document:',i, 'of', length(allEmails)))
# for each row in the annotation information, extract the term by index
POSls <- list()
  for(j in 1:nrow(y)){
    annoChars <- ((substr(x,y[j,2],y[j,3]))) #substring position
# Organize information in data frame
z <- data.frame(doc_id = i,
                    type     = y[j,1],
                    start    = y[j,2],
                    end      = y[j,3],
                    features = y[j,4],
                    text     = as.character(annoChars))
    POSls[[j]] <- z
    #print(paste('getting POS:', j))
  }
 # Bind each documents annotations & terms from loop into a single DF
  docPOS       <- do.call(rbind, POSls)
# So each document will have an individual DF of terms, and annotations as a list element
  allData[[i]] <- docPOS
}
```

Now we can subset for each document

```{r}
people       <- pblapply(allData, subset, grepl("*person", features))
locaction    <- pblapply(allData, subset, grepl("*location", features))
organization <- pblapply(allData, subset, grepl("*organization", features))

# people

# locaction #note location is misspelled, but it is also a dplyr function

# organization

### Or if you prefer to work with flat objects make it a data frame w/all info
POSdf <- do.call(rbind, allData)

# Subsetting example w/2 conditions; people found in email 1
subset(POSdf, POSdf$doc_id ==1 & grepl("*person", POSdf$features) == T)
```

Observation: These codes take incredible long to run. 

Annotate Entities Process

This final data frame contains not only persons, locations, and organizations, but also each detected sentence, word, and part of speech. To complete the named entity analysis, it is advisable to subset the data frame for specific features. The code below used the logical grep1 as a subset parameter for each entity class.

```{r}
annDF

subset(annDF$words, grepl("*people", annDF$features) == T)
subset(annDF$words, grepl("*locaction", annDF$features) == T)
subset(annDF$words, grepl("*organization", annDF$features) == T)

person
```

To define an annotation sequence, create a list with the specific opeNLP models

```{r}
annotate.entities <- function(doc, annotation.pipeline) {
  annotations <- annotate(doc, annotation.pipeline)
  AnnotatedPlainTextDocument(doc, annotations)
}

ner.pipeline <- list(
  Maxent_Sent_Token_Annotator(),
  Maxent_Word_Token_Annotator(),
  Maxent_POS_Tag_Annotator(),
  Maxent_Entity_Annotator(kind = "person"),
  Maxent_Entity_Annotator(kind = "location"),
  Maxent_Entity_Annotator(kind = "organization")
)
```

Now that we have an annotation function that individually calls the models, we need to apply them to the entire email list either the lapply or the pblapply functions will work, but the pblapply is helpful because it provides a progress bar. Note, the pbapply package must be loaded.

```{r}
## all.ner <- pblapply(all.emails, annotate.entities, ner.pipeline)
all.ner <- pblapply(allEmails, annotate.entities, ner.pipeline)

## Now, we can extract the useful information and construct a data frame with entity information.
all.ner
all.ner <-  pluck(all.ner, "annotations")
all.ner <- pblapply(all.ner, as.data.frame)

all.ner[[3]][244:250,]
all.ner <- Map(function(tex,fea,id) cbind(fea, entity = substring(tex,fea$start, fea$end), file = id), all.emails, all.ner, temp)
```

*This code did not run because we were unable to install the mini condo environment. *

# *Part 3: Conducting NLP Analysis with spacyr*

Now, to practice with spacyr, let's create two sample "documents", parse them, and save in a data.table.

```{r}
txt <- c(d1 = "spaCy is great at fast natural language processing.",
         d2 = "Mr. Smith spent two years in North Carolina.")

parsedtxt <- spacy_parse(txt)

parsedtxt
```

In this analysis, we see two fields for POS tags (pos and entity). In this case the pos field returns the parts of speech as tagged using the Penn Treebank tagset. You may adjust the argument for the spacy_parse() function to determine what you get back. For example, you may choose to not generate the lemma or entity fields with the code chunk below:

```{r}
spacy_parse(txt, tag = TRUE, entity = FALSE, lemma = FALSE)
```

You may also parse and POS tag documents in multiple languages using over 70 other language models (some of these include: German (de), Spanish (es), Portugese (pt), French (fr), Italian (it), Dutch(nl), and many others: See: <https://spacy.io/usage/models#languages> for a complete list)

You may also use spacyr to directly parse text to tokenize, using the spacy_tokenize() function; this approach is designed to match the tokens() functions within quanteda

```{r}
spacy_tokenize(txt)
```

The default returns a named list (where the document name, eg d1, d2 is the list element name). Or you may specify to output to a data.frame. Either way, make sure you have dplyr loaded.

```{r}
library(dplyr)

spacy_tokenize(txt, remove_punct = TRUE, output = "data.frame") %>%
  tail()
```

You may also use spacyr to extract named entities from the parsed text.

```{r}
parsedtxt <- spacy_parse(txt, lemma = FALSE, entity = TRUE, nounphrase = TRUE)
entity_extract(parsedtxt)
```

The following approach lets you extract the "extended" entity set:

```{r}
entity_extract(parsedtxt, type = "all")
```

Another interesting possibility is to "consolidate" multi-word entities into single tokens using the entity_consolidate() functions

```{r}
entity_consolidate(parsedtxt) %>%
  tail()
```

Similarly, spacyr can extract noun phrases

```{r}
nounphrase_extract(parsedtxt)
```

Noun phrases may also be consolidated

```{r}
nounphrase_consolidate(parsedtxt)
```

To only extract entities without parsing the entire text, you may use the spacy_extract_entity() function:

```{r}
spacy_extract_entity(txt)
```

Similarly, to extract noun phrases without parsing the entire text, you may use the spacy_extract_nounphrases() function:

```{r}
spacy_extract_nounphrases(txt)
```

For detailed parsing of syntactic dependencies, you may use the dependency=TRUE option in the argument

```{r}
spacy_parse(txt, dependency = TRUE, lemma = FALSE, pos = FALSE)
```

# You may also extract additional attributes of spaCy tokens using the additional_attributes option in the argument

```{r}
spacy_parse("I have six email addresses, including me@mymail.com.", 
            additional_attributes = c("like_num", "like_email"),
            lemma = FALSE, pos = FALSE, entity = FALSE)
```

Take a moment to review this output and discuss why this was produced from the parsing request.

You may also integrate the output from spacyr directly into quanteda. For example

```{r}
library(quanteda, warn.conflicts = FALSE, quietly = TRUE)

# To identify the names of the documents
docnames(parsedtxt)

# To count the number of tokens in the documents
ntoken(parsedtxt)

# To count the number or types of tokens
ntype(parsedtxt)
```

You may also convert tokens in spacyr, which have tokenizers that are "smarter" than the purely syntactic pattern-based parsers used by quanteda

```{r}
parsedtxt <- spacy_parse(txt, pos = TRUE, tag = TRUE)
as.tokens(parsedtxt)
```

If you want to select only nouns, using "glob" pattern matching with quanteda, you may use the tokens_select() function:

```{r}
spacy_parse("The cat in the hat ate green eggs and ham.", pos = TRUE) %>%
  as.tokens(include_pos = "pos") %>%
  tokens_select(pattern = c("*/NOUN"))
```

You may also directly convert the spaCy-based tokens:

```{r}
spacy_tokenize(txt) %>%
  as.tokens()
```

You may also do this for sentences, for which spaCy is very smart:

```{r}
txt2 <- "A Ph.D. in Washington D.C.  Mr. Smith went to Washington."
spacy_tokenize(txt2, what = "sentence") %>%
  as.tokens()
```

This also works well with entity recognition

```{r}
spacy_parse(txt, entity = TRUE) %>%
  entity_consolidate() %>%
  as.tokens() %>% 
  head(1)
```

The spacyr package also works well with tidytext

```{r}
library("tidytext")

unnest_tokens(parsedtxt, word, token) %>%
  dplyr::anti_join(stop_words)
```

We can then use POS filtering using dplyr

```{r}
spacy_parse("The cat in the hat ate green eggs and ham.", pos = TRUE) %>%
  unnest_tokens(word, token) %>%
  dplyr::filter(pos == "NOUN")
```

Since the spacy_initialize() attaches a background process of spaCy in python space, it takes up a significant amount of memory (especially when using a large language model such as: en_core_web_lg). So, when you no longer need the connection to spaCy, you may remove the spaCy object by calling the spacy_finalize() function.

```{r}
spacy_finalize()
```

And, when you are ready to reattach the back-end spaCy object, you call spacy_initialize() again.

```{r}
# spacy_initialize()
```

## *** End of Lab ***

